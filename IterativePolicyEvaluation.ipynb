{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IterativePolicyEvaluation.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "7fcRdZgc49A_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Iterative Policy Evaluation\n",
        "\n",
        "Policy evaluaton: Evaluate to state-value fucton (v(pi)) of Policy (pi), called to prediction problem\n",
        "To do iterative policy evaluation, it required to full backup method which uses all previous values of actions to evaluate the new steps staus of each state.\n"
      ]
    },
    {
      "metadata": {
        "id": "Nl_llO8uWfon",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# https://deeplearningcourses.com/c/artificial-intelligence-reinforcement-learning-in-python\n",
        "# https://www.udemy.com/artificial-intelligence-reinforcement-learning-in-python\n",
        "from __future__ import print_function, division\n",
        "from builtins import range\n",
        "# Note: you may need to update your version of future\n",
        "# sudo pip install -U future\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class Grid: # Environment\n",
        "  def __init__(self, width, height, start):\n",
        "    self.width = width\n",
        "    self.height = height\n",
        "    self.i = start[0]\n",
        "    self.j = start[1]\n",
        "\n",
        "  def set(self, rewards, actions):\n",
        "    # rewards should be a dict of: (i, j): r (row, col): reward\n",
        "    # actions should be a dict of: (i, j): A (row, col): list of possible actions\n",
        "    self.rewards = rewards\n",
        "    self.actions = actions\n",
        "\n",
        "  def set_state(self, s):\n",
        "    self.i = s[0]\n",
        "    self.j = s[1]\n",
        "\n",
        "  def current_state(self):\n",
        "    return (self.i, self.j)\n",
        "\n",
        "  def is_terminal(self, s):\n",
        "    return s not in self.actions\n",
        "\n",
        "  def move(self, action):\n",
        "    # check if legal move first\n",
        "    if action in self.actions[(self.i, self.j)]:\n",
        "      if action == 'U':\n",
        "        self.i -= 1\n",
        "      elif action == 'D':\n",
        "        self.i += 1\n",
        "      elif action == 'R':\n",
        "        self.j += 1\n",
        "      elif action == 'L':\n",
        "        self.j -= 1\n",
        "    # return a reward (if any)\n",
        "    return self.rewards.get((self.i, self.j), 0)\n",
        "\n",
        "  def undo_move(self, action):\n",
        "    # these are the opposite of what U/D/L/R should normally do\n",
        "    if action == 'U':\n",
        "      self.i += 1\n",
        "    elif action == 'D':\n",
        "      self.i -= 1\n",
        "    elif action == 'R':\n",
        "      self.j -= 1\n",
        "    elif action == 'L':\n",
        "      self.j += 1\n",
        "    # raise an exception if we arrive somewhere we shouldn't be\n",
        "    # should never happen\n",
        "    assert(self.current_state() in self.all_states())\n",
        "\n",
        "  def game_over(self):\n",
        "    # returns true if game is over, else false\n",
        "    # true if we are in a state where no actions are possible\n",
        "    return (self.i, self.j) not in self.actions\n",
        "\n",
        "  def all_states(self):\n",
        "    # possibly buggy but simple way to get all states\n",
        "    # either a position that has possible next actions\n",
        "    # or a position that yields a reward\n",
        "    return set(self.actions.keys()) | set(self.rewards.keys())\n",
        "\n",
        "\n",
        "def standard_grid():\n",
        "  # define a grid that describes the reward for arriving at each state\n",
        "  # and possible actions at each state\n",
        "  # the grid looks like this\n",
        "  # x means you can't go there\n",
        "  # s means start position\n",
        "  # number means reward at that state\n",
        "  # .  .  .  1\n",
        "  # .  x  . -1\n",
        "  # s  .  .  .\n",
        "  g = Grid(3, 4, (2, 0))\n",
        "  rewards = {(0, 3): 1, (1, 3): -1}\n",
        "  actions = {\n",
        "    (0, 0): ('D', 'R'),\n",
        "    (0, 1): ('L', 'R'),\n",
        "    (0, 2): ('L', 'D', 'R'),\n",
        "    (1, 0): ('U', 'D'),\n",
        "    (1, 2): ('U', 'D', 'R'),\n",
        "    (2, 0): ('U', 'R'),\n",
        "    (2, 1): ('L', 'R'),\n",
        "    (2, 2): ('L', 'R', 'U'),\n",
        "    (2, 3): ('L', 'U'),\n",
        "  }\n",
        "  g.set(rewards, actions)\n",
        "  return g\n",
        "\n",
        "\n",
        "def negative_grid(step_cost=-0.1):\n",
        "  # in this game we want to try to minimize the number of moves\n",
        "  # so we will penalize every move\n",
        "  g = standard_grid()\n",
        "  g.rewards.update({\n",
        "    (0, 0): step_cost,\n",
        "    (0, 1): step_cost,\n",
        "    (0, 2): step_cost,\n",
        "    (1, 0): step_cost,\n",
        "    (1, 2): step_cost,\n",
        "    (2, 0): step_cost,\n",
        "    (2, 1): step_cost,\n",
        "    (2, 2): step_cost,\n",
        "    (2, 3): step_cost,\n",
        "  })\n",
        "  return g\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "a25478IvWfow",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "e5420795-36d6-40c3-eee9-50b100b43ecf",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1530768605036,
          "user_tz": 360,
          "elapsed": 325,
          "user": {
            "displayName": "Kay Park",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "115858547447612857691"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# https://deeplearningcourses.com/c/artificial-intelligence-reinforcement-learning-in-python\n",
        "# https://www.udemy.com/artificial-intelligence-reinforcement-learning-in-python\n",
        "from __future__ import print_function, division\n",
        "from builtins import range\n",
        "# Note: you may need to update your version of future\n",
        "# sudo pip install -U future\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "SMALL_ENOUGH = 1e-3 # threshold for convergence\n",
        "\n",
        "def print_values(V, g):\n",
        "  for i in range(g.width):\n",
        "    print(\"---------------------------\")\n",
        "    for j in range(g.height):\n",
        "      v = V.get((i,j), 0)\n",
        "      if v >= 0:\n",
        "        print(\" %.2f|\" % v, end=\"\")\n",
        "      else:\n",
        "        print(\"%.2f|\" % v, end=\"\") # -ve sign takes up an extra space\n",
        "    print(\"\")\n",
        "\n",
        "\n",
        "def print_policy(P, g):\n",
        "  for i in range(g.width):\n",
        "    print(\"---------------------------\")\n",
        "    for j in range(g.height):\n",
        "      a = P.get((i,j), ' ')\n",
        "      print(\"  %s  |\" % a, end=\"\")\n",
        "    print(\"\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  # iterative policy evaluation\n",
        "  # given a policy, let's find it's value function V(s)\n",
        "  # we will do this for both a uniform random policy and fixed policy\n",
        "  # NOTE:\n",
        "  # there are 2 sources of randomness\n",
        "  # p(a|s) - deciding what action to take given the state\n",
        "  # p(s',r|s,a) - the next state and reward given your action-state pair\n",
        "  # we are only modeling p(a|s) = uniform\n",
        "  # how would the code change if p(s',r|s,a) is not deterministic?\n",
        "  grid = standard_grid()\n",
        "\n",
        "  # states will be positions (i,j)\n",
        "  # simpler than tic-tac-toe because we only have one \"game piece\"\n",
        "  # that can only be at one position at a time\n",
        "  states = grid.all_states()\n",
        "\n",
        "  ### uniformly random actions ###\n",
        "  # initialize V(s) = 0\n",
        "  V = {}\n",
        "  for s in states:\n",
        "    V[s] = 0\n",
        "  gamma = 1.0 # discount factor\n",
        "  # repeat until convergence\n",
        "  while True:\n",
        "    biggest_change = 0\n",
        "    for s in states:\n",
        "      old_v = V[s]\n",
        "\n",
        "      # V(s) only has value if it's not a terminal state\n",
        "      if s in grid.actions:\n",
        "\n",
        "        new_v = 0 # we will accumulate the answer\n",
        "        p_a = 1.0 / len(grid.actions[s]) # each action has equal probability\n",
        "        for a in grid.actions[s]:\n",
        "          grid.set_state(s)\n",
        "          r = grid.move(a)\n",
        "          new_v += p_a * (r + gamma * V[grid.current_state()])\n",
        "        V[s] = new_v\n",
        "        biggest_change = max(biggest_change, np.abs(old_v - V[s]))\n",
        "\n",
        "    if biggest_change < SMALL_ENOUGH:\n",
        "      break\n",
        "  print(\"values for uniformly random actions:\")\n",
        "  print_values(V, grid)\n",
        "  print(\"\\n\\n\")\n",
        "\n",
        "  ### fixed policy ###\n",
        "  policy = {\n",
        "    (2, 0): 'U',\n",
        "    (1, 0): 'U',\n",
        "    (0, 0): 'R',\n",
        "    (0, 1): 'R',\n",
        "    (0, 2): 'R',\n",
        "    (1, 2): 'R',\n",
        "    (2, 1): 'R',\n",
        "    (2, 2): 'R',\n",
        "    (2, 3): 'U',\n",
        "  }\n",
        "  print_policy(policy, grid)\n",
        "\n",
        "  # initialize V(s) = 0\n",
        "  V = {}\n",
        "  for s in states:\n",
        "    V[s] = 0\n",
        "\n",
        "  # let's see how V(s) changes as we get further away from the reward\n",
        "  gamma = 0.9 # discount factor\n",
        "\n",
        "  # repeat until convergence\n",
        "  while True:\n",
        "    biggest_change = 0\n",
        "    for s in states:\n",
        "      old_v = V[s]\n",
        "\n",
        "      # V(s) only has value if it's not a terminal state\n",
        "      if s in policy:\n",
        "        a = policy[s]\n",
        "        grid.set_state(s)\n",
        "        r = grid.move(a)\n",
        "        V[s] = r + gamma * V[grid.current_state()] #equation in the pdf file \n",
        "        # discounted standar, negative grid, change maybe\n",
        "        # negative grid have penalty\n",
        "        # standdard grid is zero\n",
        "        biggest_change = max(biggest_change, np.abs(old_v - V[s]))\n",
        "\n",
        "    if biggest_change < SMALL_ENOUGH:\n",
        "      break\n",
        "  print(\"values for fixed policy:\")\n",
        "  print_values(V, grid)\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "values for uniformly random actions:\n",
            "---------------------------\n",
            "-0.03| 0.09| 0.22| 0.00|\n",
            "---------------------------\n",
            "-0.16| 0.00|-0.44| 0.00|\n",
            "---------------------------\n",
            "-0.29|-0.41|-0.54|-0.77|\n",
            "\n",
            "\n",
            "\n",
            "---------------------------\n",
            "  R  |  R  |  R  |     |\n",
            "---------------------------\n",
            "  U  |     |  R  |     |\n",
            "---------------------------\n",
            "  U  |  R  |  R  |  U  |\n",
            "values for fixed policy:\n",
            "---------------------------\n",
            " 0.81| 0.90| 1.00| 0.00|\n",
            "---------------------------\n",
            " 0.73| 0.00|-1.00| 0.00|\n",
            "---------------------------\n",
            " 0.66|-0.81|-0.90|-1.00|\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "KnGEN85Apmv-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "lower discounted factor: lower reward in the future\n",
        "\n",
        "Dicounted Factor means change in the value of reward according to time\n",
        "* 0: myopic\n",
        "* 1: future oriented"
      ]
    },
    {
      "metadata": {
        "id": "hUxr44GPpmXA",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "73b4f7b6-9061-4050-e2c0-f8c6f05deeb0",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1530768610484,
          "user_tz": 360,
          "elapsed": 279,
          "user": {
            "displayName": "Kay Park",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "115858547447612857691"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# https://deeplearningcourses.com/c/artificial-intelligence-reinforcement-learning-in-python\n",
        "# https://www.udemy.com/artificial-intelligence-reinforcement-learning-in-python\n",
        "from __future__ import print_function, division\n",
        "from builtins import range\n",
        "# Note: you may need to update your version of future\n",
        "# sudo pip install -U future\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "SMALL_ENOUGH = 1e-3 # threshold for convergence\n",
        "\n",
        "def print_values(V, g):\n",
        "  for i in range(g.width):\n",
        "    print(\"---------------------------\")\n",
        "    for j in range(g.height):\n",
        "      v = V.get((i,j), 0)\n",
        "      if v >= 0:\n",
        "        print(\" %.2f|\" % v, end=\"\")\n",
        "      else:\n",
        "        print(\"%.2f|\" % v, end=\"\") # -ve sign takes up an extra space\n",
        "    print(\"\")\n",
        "\n",
        "\n",
        "def print_policy(P, g):\n",
        "  for i in range(g.width):\n",
        "    print(\"---------------------------\")\n",
        "    for j in range(g.height):\n",
        "      a = P.get((i,j), ' ')\n",
        "      print(\"  %s  |\" % a, end=\"\")\n",
        "    print(\"\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  # iterative policy evaluation\n",
        "  # given a policy, let's find it's value function V(s)\n",
        "  # we will do this for both a uniform random policy and fixed policy\n",
        "  # NOTE:\n",
        "  # there are 2 sources of randomness\n",
        "  # p(a|s) - deciding what action to take given the state\n",
        "  # p(s',r|s,a) - the next state and reward given your action-state pair\n",
        "  # we are only modeling p(a|s) = uniform\n",
        "  # how would the code change if p(s',r|s,a) is not deterministic?\n",
        "  grid = standard_grid()\n",
        "\n",
        "  # states will be positions (i,j)\n",
        "  # simpler than tic-tac-toe because we only have one \"game piece\"\n",
        "  # that can only be at one position at a time\n",
        "  states = grid.all_states()\n",
        "\n",
        "  ### uniformly random actions ###\n",
        "  # initialize V(s) = 0\n",
        "  V = {}\n",
        "  for s in states:\n",
        "    V[s] = 0\n",
        "  gamma = 1.0 # discount factor\n",
        "  # repeat until convergence\n",
        "  while True:\n",
        "    biggest_change = 0\n",
        "    for s in states:\n",
        "      old_v = V[s]\n",
        "\n",
        "      # V(s) only has value if it's not a terminal state\n",
        "      if s in grid.actions:\n",
        "\n",
        "        new_v = 0 # we will accumulate the answer\n",
        "        p_a = 1.0 / len(grid.actions[s]) # each action has equal probability\n",
        "        for a in grid.actions[s]:\n",
        "          grid.set_state(s)\n",
        "          r = grid.move(a)\n",
        "          new_v += p_a * (r + gamma * V[grid.current_state()])\n",
        "        V[s] = new_v\n",
        "        biggest_change = max(biggest_change, np.abs(old_v - V[s]))\n",
        "\n",
        "    if biggest_change < SMALL_ENOUGH:\n",
        "      break\n",
        "  print(\"values for uniformly random actions:\")\n",
        "  print_values(V, grid)\n",
        "  print(\"\\n\\n\")\n",
        "\n",
        "  ### fixed policy ###\n",
        "  policy = {\n",
        "    (2, 0): 'U',\n",
        "    (1, 0): 'U',\n",
        "    (0, 0): 'R',\n",
        "    (0, 1): 'R',\n",
        "    (0, 2): 'R',\n",
        "    (1, 2): 'R',\n",
        "    (2, 1): 'R',\n",
        "    (2, 2): 'R',\n",
        "    (2, 3): 'U',\n",
        "  }\n",
        "  print_policy(policy, grid)\n",
        "\n",
        "  # initialize V(s) = 0\n",
        "  V = {}\n",
        "  for s in states:\n",
        "    V[s] = 0\n",
        "\n",
        "  # let's see how V(s) changes as we get further away from the reward\n",
        "  gamma = 1.0 # discount factor\n",
        "\n",
        "  # repeat until convergence\n",
        "  while True:\n",
        "    biggest_change = 0\n",
        "    for s in states:\n",
        "      old_v = V[s]\n",
        "\n",
        "      # V(s) only has value if it's not a terminal state\n",
        "      if s in policy:\n",
        "        a = policy[s]\n",
        "        grid.set_state(s)\n",
        "        r = grid.move(a)\n",
        "        V[s] = r + gamma * V[grid.current_state()] #equation in the pdf file \n",
        "        # discounted standar, negative grid, change maybe\n",
        "        # negative grid have penalty\n",
        "        # standdard grid is zero\n",
        "        biggest_change = max(biggest_change, np.abs(old_v - V[s]))\n",
        "\n",
        "    if biggest_change < SMALL_ENOUGH:\n",
        "      break\n",
        "  print(\"values for fixed policy:\")\n",
        "  print_values(V, grid)\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "values for uniformly random actions:\n",
            "---------------------------\n",
            "-0.03| 0.09| 0.22| 0.00|\n",
            "---------------------------\n",
            "-0.16| 0.00|-0.44| 0.00|\n",
            "---------------------------\n",
            "-0.29|-0.41|-0.54|-0.77|\n",
            "\n",
            "\n",
            "\n",
            "---------------------------\n",
            "  R  |  R  |  R  |     |\n",
            "---------------------------\n",
            "  U  |     |  R  |     |\n",
            "---------------------------\n",
            "  U  |  R  |  R  |  U  |\n",
            "values for fixed policy:\n",
            "---------------------------\n",
            " 1.00| 1.00| 1.00| 0.00|\n",
            "---------------------------\n",
            " 1.00| 0.00|-1.00| 0.00|\n",
            "---------------------------\n",
            " 1.00|-1.00|-1.00|-1.00|\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "FFp6c_wc47gh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "IZ9RRvMtWfo9",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "082e4b7b-98c4-4986-c6ee-66a755ae3725",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1530768881051,
          "user_tz": 360,
          "elapsed": 262,
          "user": {
            "displayName": "Kay Park",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "115858547447612857691"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# https://deeplearningcourses.com/c/artificial-intelligence-reinforcement-learning-in-python\n",
        "# https://www.udemy.com/artificial-intelligence-reinforcement-learning-in-python\n",
        "from __future__ import print_function, division\n",
        "from builtins import range\n",
        "# Note: you may need to update your version of future\n",
        "# sudo pip install -U future\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "SMALL_ENOUGH = 1e-3 # threshold for convergence\n",
        "\n",
        "def print_values(V, g):\n",
        "  for i in range(g.width):\n",
        "    print(\"---------------------------\")\n",
        "    for j in range(g.height):\n",
        "      v = V.get((i,j), 0)\n",
        "      if v >= 0:\n",
        "        print(\" %.2f|\" % v, end=\"\")\n",
        "      else:\n",
        "        print(\"%.2f|\" % v, end=\"\") # -ve sign takes up an extra space\n",
        "    print(\"\")\n",
        "\n",
        "\n",
        "def print_policy(P, g):\n",
        "  for i in range(g.width):\n",
        "    print(\"---------------------------\")\n",
        "    for j in range(g.height):\n",
        "      a = P.get((i,j), ' ')\n",
        "      print(\"  %s  |\" % a, end=\"\")\n",
        "    print(\"\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  # iterative policy evaluation\n",
        "  # given a policy, let's find it's value function V(s)\n",
        "  # we will do this for both a uniform random policy and fixed policy\n",
        "  # NOTE:\n",
        "  # there are 2 sources of randomness\n",
        "  # p(a|s) - deciding what action to take given the state\n",
        "  # p(s',r|s,a) - the next state and reward given your action-state pair\n",
        "  # we are only modeling p(a|s) = uniform\n",
        "  # how would the code change if p(s',r|s,a) is not deterministic?\n",
        "  grid = standard_grid()\n",
        "\n",
        "  # states will be positions (i,j)\n",
        "  # simpler than tic-tac-toe because we only have one \"game piece\"\n",
        "  # that can only be at one position at a time\n",
        "  states = grid.all_states()\n",
        "\n",
        "  ### uniformly random actions ###\n",
        "  # initialize V(s) = 0\n",
        "  V = {}\n",
        "  for s in states:\n",
        "    V[s] = 0\n",
        "  gamma = 1.0 # discount factor\n",
        "  # repeat until convergence\n",
        "  while True:\n",
        "    biggest_change = 0\n",
        "    for s in states:\n",
        "      old_v = V[s]\n",
        "\n",
        "      # V(s) only has value if it's not a terminal state\n",
        "      if s in grid.actions:\n",
        "\n",
        "        new_v = 0 # we will accumulate the answer\n",
        "        p_a = 1.0 / len(grid.actions[s]) # each action has equal probability\n",
        "        for a in grid.actions[s]:\n",
        "          grid.set_state(s)\n",
        "          r = grid.move(a)\n",
        "          new_v += p_a * (r + gamma * V[grid.current_state()])\n",
        "        V[s] = new_v\n",
        "        biggest_change = max(biggest_change, np.abs(old_v - V[s]))\n",
        "\n",
        "    if biggest_change < SMALL_ENOUGH:\n",
        "      break\n",
        "  print(\"values for uniformly random actions:\")\n",
        "  print_values(V, grid)\n",
        "  print(\"\\n\\n\")\n",
        "\n",
        "  ### fixed policy ###\n",
        "  policy = {\n",
        "    (2, 0): 'U',\n",
        "    (1, 0): 'U',\n",
        "    (0, 0): 'R',\n",
        "    (0, 1): 'R',\n",
        "    (0, 2): 'R',\n",
        "    (1, 2): 'R',\n",
        "    (2, 1): 'R',\n",
        "    (2, 2): 'R',\n",
        "    (2, 3): 'U',\n",
        "  }\n",
        "  print_policy(policy, grid)\n",
        "\n",
        "  # initialize V(s) = 0\n",
        "  V = {}\n",
        "  for s in states:\n",
        "    V[s] = 0\n",
        "\n",
        "  # let's see how V(s) changes as we get further away from the reward\n",
        "  gamma = 0.0 # discount factor\n",
        "\n",
        "  # repeat until convergence\n",
        "  while True:\n",
        "    biggest_change = 0\n",
        "    for s in states:\n",
        "      old_v = V[s]\n",
        "\n",
        "      # V(s) only has value if it's not a terminal state\n",
        "      if s in policy:\n",
        "        a = policy[s]\n",
        "        grid.set_state(s)\n",
        "        r = grid.move(a)\n",
        "        V[s] = r + gamma * V[grid.current_state()] #equation in the pdf file \n",
        "        # discounted standar, negative grid, change maybe\n",
        "        # negative grid have penalty\n",
        "        # standdard grid is zero\n",
        "        biggest_change = max(biggest_change, np.abs(old_v - V[s]))\n",
        "\n",
        "    if biggest_change < SMALL_ENOUGH:\n",
        "      break\n",
        "  print(\"values for fixed policy:\")\n",
        "  print_values(V, grid)\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "values for uniformly random actions:\n",
            "---------------------------\n",
            "-0.03| 0.09| 0.22| 0.00|\n",
            "---------------------------\n",
            "-0.16| 0.00|-0.44| 0.00|\n",
            "---------------------------\n",
            "-0.29|-0.41|-0.54|-0.77|\n",
            "\n",
            "\n",
            "\n",
            "---------------------------\n",
            "  R  |  R  |  R  |     |\n",
            "---------------------------\n",
            "  U  |     |  R  |     |\n",
            "---------------------------\n",
            "  U  |  R  |  R  |  U  |\n",
            "values for fixed policy:\n",
            "---------------------------\n",
            " 0.00| 0.00| 1.00| 0.00|\n",
            "---------------------------\n",
            " 0.00| 0.00|-1.00| 0.00|\n",
            "---------------------------\n",
            " 0.00| 0.00| 0.00|-1.00|\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "QczO2KDYsbUD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Change fixed policy to optimal policy"
      ]
    },
    {
      "metadata": {
        "id": "nK-tyekkqxx0",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "d19dac6d-8a77-4c54-c48f-01980dea97f9",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1530768979323,
          "user_tz": 360,
          "elapsed": 293,
          "user": {
            "displayName": "Kay Park",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "115858547447612857691"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# https://deeplearningcourses.com/c/artificial-intelligence-reinforcement-learning-in-python\n",
        "# https://www.udemy.com/artificial-intelligence-reinforcement-learning-in-python\n",
        "from __future__ import print_function, division\n",
        "from builtins import range\n",
        "# Note: you may need to update your version of future\n",
        "# sudo pip install -U future\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "SMALL_ENOUGH = 1e-3 # threshold for convergence\n",
        "\n",
        "def print_values(V, g):\n",
        "  for i in range(g.width):\n",
        "    print(\"---------------------------\")\n",
        "    for j in range(g.height):\n",
        "      v = V.get((i,j), 0)\n",
        "      if v >= 0:\n",
        "        print(\" %.2f|\" % v, end=\"\")\n",
        "      else:\n",
        "        print(\"%.2f|\" % v, end=\"\") # -ve sign takes up an extra space\n",
        "    print(\"\")\n",
        "\n",
        "\n",
        "def print_policy(P, g):\n",
        "  for i in range(g.width):\n",
        "    print(\"---------------------------\")\n",
        "    for j in range(g.height):\n",
        "      a = P.get((i,j), ' ')\n",
        "      print(\"  %s  |\" % a, end=\"\")\n",
        "    print(\"\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  # iterative policy evaluation\n",
        "  # given a policy, let's find it's value function V(s)\n",
        "  # we will do this for both a uniform random policy and fixed policy\n",
        "  # NOTE:\n",
        "  # there are 2 sources of randomness\n",
        "  # p(a|s) - deciding what action to take given the state\n",
        "  # p(s',r|s,a) - the next state and reward given your action-state pair\n",
        "  # we are only modeling p(a|s) = uniform\n",
        "  # how would the code change if p(s',r|s,a) is not deterministic?\n",
        "  grid = standard_grid()\n",
        "\n",
        "  # states will be positions (i,j)\n",
        "  # simpler than tic-tac-toe because we only have one \"game piece\"\n",
        "  # that can only be at one position at a time\n",
        "  states = grid.all_states()\n",
        "\n",
        "  ### uniformly random actions ###\n",
        "  # initialize V(s) = 0\n",
        "  V = {}\n",
        "  for s in states:\n",
        "    V[s] = 0\n",
        "  gamma = 1.0 # discount factor\n",
        "  # repeat until convergence\n",
        "  while True:\n",
        "    biggest_change = 0\n",
        "    for s in states:\n",
        "      old_v = V[s]\n",
        "\n",
        "      # V(s) only has value if it's not a terminal state\n",
        "      if s in grid.actions:\n",
        "\n",
        "        new_v = 0 # we will accumulate the answer\n",
        "        p_a = 1.0 / len(grid.actions[s]) # each action has equal probability\n",
        "        for a in grid.actions[s]:\n",
        "          grid.set_state(s)\n",
        "          r = grid.move(a)\n",
        "          new_v += p_a * (r + gamma * V[grid.current_state()])\n",
        "        V[s] = new_v\n",
        "        biggest_change = max(biggest_change, np.abs(old_v - V[s]))\n",
        "\n",
        "    if biggest_change < SMALL_ENOUGH:\n",
        "      break\n",
        "  print(\"values for uniformly random actions:\")\n",
        "  print_values(V, grid)\n",
        "  print(\"\\n\\n\")\n",
        "\n",
        "  ### fixed policy ###\n",
        "  policy = {\n",
        "    (2, 0): 'U',\n",
        "    (1, 0): 'U',\n",
        "    (0, 0): 'R',\n",
        "    (0, 1): 'R',\n",
        "    (0, 2): 'R',\n",
        "    (1, 2): 'R',\n",
        "    (2, 1): 'R',\n",
        "    (2, 2): 'R',\n",
        "    (2, 3): 'L',\n",
        "  }\n",
        "  print_policy(policy, grid)\n",
        "\n",
        "  # initialize V(s) = 0\n",
        "  V = {}\n",
        "  for s in states:\n",
        "    V[s] = 0\n",
        "\n",
        "  # let's see how V(s) changes as we get further away from the reward\n",
        "  gamma = 0.9 # discount factor\n",
        "\n",
        "  # repeat until convergence\n",
        "  while True:\n",
        "    biggest_change = 0\n",
        "    for s in states:\n",
        "      old_v = V[s]\n",
        "\n",
        "      # V(s) only has value if it's not a terminal state\n",
        "      if s in policy:\n",
        "        a = policy[s]\n",
        "        grid.set_state(s)\n",
        "        r = grid.move(a)\n",
        "        V[s] = r + gamma * V[grid.current_state()] #equation in the pdf file \n",
        "        # discounted standar, negative grid, change maybe\n",
        "        # negative grid have penalty\n",
        "        # standdard grid is zero\n",
        "        biggest_change = max(biggest_change, np.abs(old_v - V[s]))\n",
        "\n",
        "    if biggest_change < SMALL_ENOUGH:\n",
        "      break\n",
        "  print(\"values for fixed policy:\")\n",
        "  print_values(V, grid)\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "values for uniformly random actions:\n",
            "---------------------------\n",
            "-0.03| 0.09| 0.22| 0.00|\n",
            "---------------------------\n",
            "-0.16| 0.00|-0.44| 0.00|\n",
            "---------------------------\n",
            "-0.29|-0.41|-0.54|-0.77|\n",
            "\n",
            "\n",
            "\n",
            "---------------------------\n",
            "  R  |  R  |  R  |     |\n",
            "---------------------------\n",
            "  U  |     |  R  |     |\n",
            "---------------------------\n",
            "  U  |  R  |  R  |  L  |\n",
            "values for fixed policy:\n",
            "---------------------------\n",
            " 0.81| 0.90| 1.00| 0.00|\n",
            "---------------------------\n",
            " 0.73| 0.00|-1.00| 0.00|\n",
            "---------------------------\n",
            " 0.66| 0.00| 0.00| 0.00|\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "S1XQuuCps3sq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Change fixed policy to optimal policy without discounted factor"
      ]
    },
    {
      "metadata": {
        "id": "5OregmZBseN7",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "390c6860-db48-4a06-c316-1d02db6bf5dd",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1530768996868,
          "user_tz": 360,
          "elapsed": 254,
          "user": {
            "displayName": "Kay Park",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "115858547447612857691"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# https://deeplearningcourses.com/c/artificial-intelligence-reinforcement-learning-in-python\n",
        "# https://www.udemy.com/artificial-intelligence-reinforcement-learning-in-python\n",
        "from __future__ import print_function, division\n",
        "from builtins import range\n",
        "# Note: you may need to update your version of future\n",
        "# sudo pip install -U future\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "SMALL_ENOUGH = 1e-3 # threshold for convergence\n",
        "\n",
        "def print_values(V, g):\n",
        "  for i in range(g.width):\n",
        "    print(\"---------------------------\")\n",
        "    for j in range(g.height):\n",
        "      v = V.get((i,j), 0)\n",
        "      if v >= 0:\n",
        "        print(\" %.2f|\" % v, end=\"\")\n",
        "      else:\n",
        "        print(\"%.2f|\" % v, end=\"\") # -ve sign takes up an extra space\n",
        "    print(\"\")\n",
        "\n",
        "\n",
        "def print_policy(P, g):\n",
        "  for i in range(g.width):\n",
        "    print(\"---------------------------\")\n",
        "    for j in range(g.height):\n",
        "      a = P.get((i,j), ' ')\n",
        "      print(\"  %s  |\" % a, end=\"\")\n",
        "    print(\"\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  # iterative policy evaluation\n",
        "  # given a policy, let's find it's value function V(s)\n",
        "  # we will do this for both a uniform random policy and fixed policy\n",
        "  # NOTE:\n",
        "  # there are 2 sources of randomness\n",
        "  # p(a|s) - deciding what action to take given the state\n",
        "  # p(s',r|s,a) - the next state and reward given your action-state pair\n",
        "  # we are only modeling p(a|s) = uniform\n",
        "  # how would the code change if p(s',r|s,a) is not deterministic?\n",
        "  grid = standard_grid()\n",
        "\n",
        "  # states will be positions (i,j)\n",
        "  # simpler than tic-tac-toe because we only have one \"game piece\"\n",
        "  # that can only be at one position at a time\n",
        "  states = grid.all_states()\n",
        "\n",
        "  ### uniformly random actions ###\n",
        "  # initialize V(s) = 0\n",
        "  V = {}\n",
        "  for s in states:\n",
        "    V[s] = 0\n",
        "  gamma = 1.0 # discount factor\n",
        "  # repeat until convergence\n",
        "  while True:\n",
        "    biggest_change = 0\n",
        "    for s in states:\n",
        "      old_v = V[s]\n",
        "\n",
        "      # V(s) only has value if it's not a terminal state\n",
        "      if s in grid.actions:\n",
        "\n",
        "        new_v = 0 # we will accumulate the answer\n",
        "        p_a = 1.0 / len(grid.actions[s]) # each action has equal probability\n",
        "        for a in grid.actions[s]:\n",
        "          grid.set_state(s)\n",
        "          r = grid.move(a)\n",
        "          new_v += p_a * (r + gamma * V[grid.current_state()])\n",
        "        V[s] = new_v\n",
        "        biggest_change = max(biggest_change, np.abs(old_v - V[s]))\n",
        "\n",
        "    if biggest_change < SMALL_ENOUGH:\n",
        "      break\n",
        "  print(\"values for uniformly random actions:\")\n",
        "  print_values(V, grid)\n",
        "  print(\"\\n\\n\")\n",
        "\n",
        "  ### fixed policy ###\n",
        "  policy = {\n",
        "    (2, 0): 'U',\n",
        "    (1, 0): 'U',\n",
        "    (0, 0): 'R',\n",
        "    (0, 1): 'R',\n",
        "    (0, 2): 'R',\n",
        "    (1, 2): 'R',\n",
        "    (2, 1): 'R',\n",
        "    (2, 2): 'R',\n",
        "    (2, 3): 'L',\n",
        "  }\n",
        "  print_policy(policy, grid)\n",
        "\n",
        "  # initialize V(s) = 0\n",
        "  V = {}\n",
        "  for s in states:\n",
        "    V[s] = 0\n",
        "\n",
        "  # let's see how V(s) changes as we get further away from the reward\n",
        "  gamma = 1.0 # discount factor\n",
        "\n",
        "  # repeat until convergence\n",
        "  while True:\n",
        "    biggest_change = 0\n",
        "    for s in states:\n",
        "      old_v = V[s]\n",
        "\n",
        "      # V(s) only has value if it's not a terminal state\n",
        "      if s in policy:\n",
        "        a = policy[s]\n",
        "        grid.set_state(s)\n",
        "        r = grid.move(a)\n",
        "        V[s] = r + gamma * V[grid.current_state()] #equation in the pdf file \n",
        "        # discounted standar, negative grid, change maybe\n",
        "        # negative grid have penalty\n",
        "        # standdard grid is zero\n",
        "        biggest_change = max(biggest_change, np.abs(old_v - V[s]))\n",
        "\n",
        "    if biggest_change < SMALL_ENOUGH:\n",
        "      break\n",
        "  print(\"values for fixed policy:\")\n",
        "  print_values(V, grid)\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "values for uniformly random actions:\n",
            "---------------------------\n",
            "-0.03| 0.09| 0.22| 0.00|\n",
            "---------------------------\n",
            "-0.16| 0.00|-0.44| 0.00|\n",
            "---------------------------\n",
            "-0.29|-0.41|-0.54|-0.77|\n",
            "\n",
            "\n",
            "\n",
            "---------------------------\n",
            "  R  |  R  |  R  |     |\n",
            "---------------------------\n",
            "  U  |     |  R  |     |\n",
            "---------------------------\n",
            "  U  |  R  |  R  |  L  |\n",
            "values for fixed policy:\n",
            "---------------------------\n",
            " 1.00| 1.00| 1.00| 0.00|\n",
            "---------------------------\n",
            " 1.00| 0.00|-1.00| 0.00|\n",
            "---------------------------\n",
            " 1.00| 0.00| 0.00| 0.00|\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "QKh-qjpW8-Fv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Change fixed policy to optimal policy with negative grid"
      ]
    },
    {
      "metadata": {
        "id": "Vr6k4HEl8Tez",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "36b910d3-a4f7-440f-d227-0544f2a34f3a",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1530769137742,
          "user_tz": 360,
          "elapsed": 300,
          "user": {
            "displayName": "Kay Park",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "115858547447612857691"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# https://deeplearningcourses.com/c/artificial-intelligence-reinforcement-learning-in-python\n",
        "# https://www.udemy.com/artificial-intelligence-reinforcement-learning-in-python\n",
        "from __future__ import print_function, division\n",
        "from builtins import range\n",
        "# Note: you may need to update your version of future\n",
        "# sudo pip install -U future\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "SMALL_ENOUGH = 1e-3 # threshold for convergence\n",
        "\n",
        "def print_values(V, g):\n",
        "  for i in range(g.width):\n",
        "    print(\"---------------------------\")\n",
        "    for j in range(g.height):\n",
        "      v = V.get((i,j), 0)\n",
        "      if v >= 0:\n",
        "        print(\" %.2f|\" % v, end=\"\")\n",
        "      else:\n",
        "        print(\"%.2f|\" % v, end=\"\") # -ve sign takes up an extra space\n",
        "    print(\"\")\n",
        "\n",
        "\n",
        "def print_policy(P, g):\n",
        "  for i in range(g.width):\n",
        "    print(\"---------------------------\")\n",
        "    for j in range(g.height):\n",
        "      a = P.get((i,j), ' ')\n",
        "      print(\"  %s  |\" % a, end=\"\")\n",
        "    print(\"\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  # iterative policy evaluation\n",
        "  # given a policy, let's find it's value function V(s)\n",
        "  # we will do this for both a uniform random policy and fixed policy\n",
        "  # NOTE:\n",
        "  # there are 2 sources of randomness\n",
        "  # p(a|s) - deciding what action to take given the state\n",
        "  # p(s',r|s,a) - the next state and reward given your action-state pair\n",
        "  # we are only modeling p(a|s) = uniform\n",
        "  # how would the code change if p(s',r|s,a) is not deterministic?\n",
        "  grid = negative_grid()\n",
        "\n",
        "  # states will be positions (i,j)\n",
        "  # simpler than tic-tac-toe because we only have one \"game piece\"\n",
        "  # that can only be at one position at a time\n",
        "  states = grid.all_states()\n",
        "\n",
        "  ### uniformly random actions ###\n",
        "  # initialize V(s) = 0\n",
        "  V = {}\n",
        "  for s in states:\n",
        "    V[s] = 0\n",
        "  gamma = 1.0 # discount factor\n",
        "  # repeat until convergence\n",
        "  while True:\n",
        "    biggest_change = 0\n",
        "    for s in states:\n",
        "      old_v = V[s]\n",
        "\n",
        "      # V(s) only has value if it's not a terminal state\n",
        "      if s in grid.actions:\n",
        "\n",
        "        new_v = 0 # we will accumulate the answer\n",
        "        p_a = 1.0 / len(grid.actions[s]) # each action has equal probability\n",
        "        for a in grid.actions[s]:\n",
        "          grid.set_state(s)\n",
        "          r = grid.move(a)\n",
        "          new_v += p_a * (r + gamma * V[grid.current_state()])\n",
        "        V[s] = new_v\n",
        "        biggest_change = max(biggest_change, np.abs(old_v - V[s]))\n",
        "\n",
        "    if biggest_change < SMALL_ENOUGH:\n",
        "      break\n",
        "  print(\"values for uniformly random actions:\")\n",
        "  print_values(V, grid)\n",
        "  print(\"\\n\\n\")\n",
        "\n",
        "  ### fixed policy ###\n",
        "  policy = {\n",
        "    (2, 0): 'U',\n",
        "    (1, 0): 'U',\n",
        "    (0, 0): 'R',\n",
        "    (0, 1): 'R',\n",
        "    (0, 2): 'R',\n",
        "    (1, 2): 'R',\n",
        "    (2, 1): 'R',\n",
        "    (2, 2): 'R',\n",
        "    (2, 3): 'L',\n",
        "  }\n",
        "  print_policy(policy, grid)\n",
        "\n",
        "  # initialize V(s) = 0\n",
        "  V = {}\n",
        "  for s in states:\n",
        "    V[s] = 0\n",
        "\n",
        "  # let's see how V(s) changes as we get further away from the reward\n",
        "  gamma = 0.9 # discount factor\n",
        "\n",
        "  # repeat until convergence\n",
        "  while True:\n",
        "    biggest_change = 0\n",
        "    for s in states:\n",
        "      old_v = V[s]\n",
        "\n",
        "      # V(s) only has value if it's not a terminal state\n",
        "      if s in policy:\n",
        "        a = policy[s]\n",
        "        grid.set_state(s)\n",
        "        r = grid.move(a)\n",
        "        V[s] = r + gamma * V[grid.current_state()] #equation in the pdf file \n",
        "        # discounted standar, negative grid, change maybe\n",
        "        # negative grid have penalty\n",
        "        # standdard grid is zero\n",
        "        biggest_change = max(biggest_change, np.abs(old_v - V[s]))\n",
        "\n",
        "    if biggest_change < SMALL_ENOUGH:\n",
        "      break\n",
        "  print(\"values for fixed policy:\")\n",
        "  print_values(V, grid)\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "values for uniformly random actions:\n",
            "---------------------------\n",
            "-1.60|-1.13|-0.46| 0.00|\n",
            "---------------------------\n",
            "-1.87| 0.00|-1.05| 0.00|\n",
            "---------------------------\n",
            "-1.94|-1.81|-1.48|-1.29|\n",
            "\n",
            "\n",
            "\n",
            "---------------------------\n",
            "  R  |  R  |  R  |     |\n",
            "---------------------------\n",
            "  U  |     |  R  |     |\n",
            "---------------------------\n",
            "  U  |  R  |  R  |  L  |\n",
            "values for fixed policy:\n",
            "---------------------------\n",
            " 0.62| 0.80| 1.00| 0.00|\n",
            "---------------------------\n",
            " 0.46| 0.00|-1.00| 0.00|\n",
            "---------------------------\n",
            " 0.31|-1.00|-1.00|-1.00|\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "39UBvW8n9NnN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Change fixed policy to optimal policy with negative grid under lower discounted factor"
      ]
    },
    {
      "metadata": {
        "id": "tgwN0gFj813Q",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "85ab34b0-1760-4ac0-e21a-fe1aa6bb9ad9",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1530769225159,
          "user_tz": 360,
          "elapsed": 284,
          "user": {
            "displayName": "Kay Park",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "115858547447612857691"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# https://deeplearningcourses.com/c/artificial-intelligence-reinforcement-learning-in-python\n",
        "# https://www.udemy.com/artificial-intelligence-reinforcement-learning-in-python\n",
        "from __future__ import print_function, division\n",
        "from builtins import range\n",
        "# Note: you may need to update your version of future\n",
        "# sudo pip install -U future\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "SMALL_ENOUGH = 1e-3 # threshold for convergence\n",
        "\n",
        "def print_values(V, g):\n",
        "  for i in range(g.width):\n",
        "    print(\"---------------------------\")\n",
        "    for j in range(g.height):\n",
        "      v = V.get((i,j), 0)\n",
        "      if v >= 0:\n",
        "        print(\" %.2f|\" % v, end=\"\")\n",
        "      else:\n",
        "        print(\"%.2f|\" % v, end=\"\") # -ve sign takes up an extra space\n",
        "    print(\"\")\n",
        "\n",
        "\n",
        "def print_policy(P, g):\n",
        "  for i in range(g.width):\n",
        "    print(\"---------------------------\")\n",
        "    for j in range(g.height):\n",
        "      a = P.get((i,j), ' ')\n",
        "      print(\"  %s  |\" % a, end=\"\")\n",
        "    print(\"\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  # iterative policy evaluation\n",
        "  # given a policy, let's find it's value function V(s)\n",
        "  # we will do this for both a uniform random policy and fixed policy\n",
        "  # NOTE:\n",
        "  # there are 2 sources of randomness\n",
        "  # p(a|s) - deciding what action to take given the state\n",
        "  # p(s',r|s,a) - the next state and reward given your action-state pair\n",
        "  # we are only modeling p(a|s) = uniform\n",
        "  # how would the code change if p(s',r|s,a) is not deterministic?\n",
        "  grid = negative_grid()\n",
        "\n",
        "  # states will be positions (i,j)\n",
        "  # simpler than tic-tac-toe because we only have one \"game piece\"\n",
        "  # that can only be at one position at a time\n",
        "  states = grid.all_states()\n",
        "\n",
        "  ### uniformly random actions ###\n",
        "  # initialize V(s) = 0\n",
        "  V = {}\n",
        "  for s in states:\n",
        "    V[s] = 0\n",
        "  gamma = 0.5 # discount factor\n",
        "  # repeat until convergence\n",
        "  while True:\n",
        "    biggest_change = 0\n",
        "    for s in states:\n",
        "      old_v = V[s]\n",
        "\n",
        "      # V(s) only has value if it's not a terminal state\n",
        "      if s in grid.actions:\n",
        "\n",
        "        new_v = 0 # we will accumulate the answer\n",
        "        p_a = 1.0 / len(grid.actions[s]) # each action has equal probability\n",
        "        for a in grid.actions[s]:\n",
        "          grid.set_state(s)\n",
        "          r = grid.move(a)\n",
        "          new_v += p_a * (r + gamma * V[grid.current_state()])\n",
        "        V[s] = new_v\n",
        "        biggest_change = max(biggest_change, np.abs(old_v - V[s]))\n",
        "\n",
        "    if biggest_change < SMALL_ENOUGH:\n",
        "      break\n",
        "  print(\"values for uniformly random actions:\")\n",
        "  print_values(V, grid)\n",
        "  print(\"\\n\\n\")\n",
        "\n",
        "  ### fixed policy ###\n",
        "  policy = {\n",
        "    (2, 0): 'U',\n",
        "    (1, 0): 'U',\n",
        "    (0, 0): 'R',\n",
        "    (0, 1): 'R',\n",
        "    (0, 2): 'R',\n",
        "    (1, 2): 'R',\n",
        "    (2, 1): 'R',\n",
        "    (2, 2): 'R',\n",
        "    (2, 3): 'L',\n",
        "  }\n",
        "  print_policy(policy, grid)\n",
        "\n",
        "  # initialize V(s) = 0\n",
        "  V = {}\n",
        "  for s in states:\n",
        "    V[s] = 0\n",
        "\n",
        "  # let's see how V(s) changes as we get further away from the reward\n",
        "  gamma = 0.5 # discount factor\n",
        "\n",
        "  # repeat until convergence\n",
        "  while True:\n",
        "    biggest_change = 0\n",
        "    for s in states:\n",
        "      old_v = V[s]\n",
        "\n",
        "      # V(s) only has value if it's not a terminal state\n",
        "      if s in policy:\n",
        "        a = policy[s]\n",
        "        grid.set_state(s)\n",
        "        r = grid.move(a)\n",
        "        V[s] = r + gamma * V[grid.current_state()] #equation in the pdf file \n",
        "        # discounted standar, negative grid, change maybe\n",
        "        # negative grid have penalty\n",
        "        # standdard grid is zero\n",
        "        biggest_change = max(biggest_change, np.abs(old_v - V[s]))\n",
        "\n",
        "    if biggest_change < SMALL_ENOUGH:\n",
        "      break\n",
        "  print(\"values for fixed policy:\")\n",
        "  print_values(V, grid)\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "values for uniformly random actions:\n",
            "---------------------------\n",
            "-0.17|-0.10| 0.18| 0.00|\n",
            "---------------------------\n",
            "-0.19| 0.00|-0.42| 0.00|\n",
            "---------------------------\n",
            "-0.21|-0.23|-0.31|-0.63|\n",
            "\n",
            "\n",
            "\n",
            "---------------------------\n",
            "  R  |  R  |  R  |     |\n",
            "---------------------------\n",
            "  U  |     |  R  |     |\n",
            "---------------------------\n",
            "  U  |  R  |  R  |  L  |\n",
            "values for fixed policy:\n",
            "---------------------------\n",
            " 0.10| 0.40| 1.00| 0.00|\n",
            "---------------------------\n",
            "-0.05| 0.00|-1.00| 0.00|\n",
            "---------------------------\n",
            "-0.12|-0.20|-0.20|-0.20|\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-zeAna7A9GSM",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}